{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a2a6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "mlb = MultiLabelBinarizer()\n",
    "\n",
    "dir_proj            = \"S:/Wissdaten/ZKI-PH4/sprint_mosqlimate/\"\n",
    "dir_data            = os.path.join(dir_proj, 'data')\n",
    "dir_data_processed  = os.path.join(dir_data, 'processed')\n",
    "\n",
    "def add_week_features(df):\n",
    "    \"\"\"\n",
    "    adds sin and cos values per week\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df[\"weekofyear\"] = (df['date'].dt.isocalendar().week - 1) / 52\n",
    "    df[\"week_sin\"]     = np.sin(2 * np.pi * df[\"weekofyear\"])\n",
    "    df[\"week_cos\"]     = np.cos(2 * np.pi * df[\"weekofyear\"])\n",
    "\n",
    "    df.drop(columns=['weekofyear'], inplace=True)\n",
    "    return df\n",
    "\n",
    "def pipeline_minmax_scaling(train_df, columns):\n",
    "    \"\"\"\n",
    "    takes training data and standardizes the columns specified into minmax\n",
    "    \n",
    "    returns the same dataframe with the specified columns standardized. Other columns remain untouched.\n",
    "    \"\"\"\n",
    "    params = {}\n",
    "    scaled_df = train_df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        col_min = train_df[col].min()\n",
    "        col_max = train_df[col].max()\n",
    "        params[col] = {'min': col_min, 'max': col_max}\n",
    "        \n",
    "        if col_max - col_min == 0:\n",
    "            scaled_df[col] = 0.0\n",
    "        else:\n",
    "            scaled_df[col] = (train_df[col] - col_min) / (col_max - col_min)\n",
    "    \n",
    "    return scaled_df, params\n",
    "\n",
    "def apply_minmax_scaling(val_df, columns, params):\n",
    "    \"\"\"\n",
    "    takes data and standardizes following parameters given. \n",
    "    This will be used for the validation dataset after having standardized the training dataset.\n",
    "\n",
    "    returns scaled data\n",
    "    \"\"\"\n",
    "    scaled_df = val_df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        if col not in params:\n",
    "            raise ValueError(f\"Scaling parameters for column '{col}' not found.\")\n",
    "        \n",
    "        col_min = params[col]['min']\n",
    "        col_max = params[col]['max']\n",
    "        \n",
    "        if col_max - col_min == 0:\n",
    "            scaled_df[col] = 0.0\n",
    "        else:\n",
    "            scaled_df[col] = (val_df[col] - col_min) / (col_max - col_min)\n",
    "    \n",
    "    return scaled_df\n",
    "\n",
    "def load_and_merge_rawdata(aggr_level = 'state'):\n",
    "    \"\"\"\"\n",
    "    individually loads each separate rawdata file and then merges it all, based on a given aggregation level. This is either:\n",
    "    \"state\", \"macroregion\" or \"region\"\n",
    "    \n",
    "    \"\"\"\n",
    "    aggr_level_map =   {'state'          : 'uf',\n",
    "                        'macroregion'    : 'macroregional_geocode',\n",
    "                        'region'         : 'regional_geocode'}\n",
    "\n",
    "    aggr_level_options = list(aggr_level_map.keys())\n",
    "\n",
    "    if aggr_level not in aggr_level_options:\n",
    "        raise ValueError(f'{aggr_level} not valid! options are {aggr_level_options}')\n",
    "\n",
    "    aggr_level_colname = aggr_level_map[aggr_level]\n",
    "\n",
    "    df_cases_raw                            = pd.read_csv(os.path.join(dir_data, 'dengue.csv.gz'))\n",
    "    df_cases_raw['year']                    = pd.to_datetime(df_cases_raw['date']).dt.year\n",
    "\n",
    "    df_cases_raw                            = df_cases_raw[df_cases_raw['uf']!= 'ES']\n",
    "\n",
    "    df_cases_aggr                           = df_cases_raw.groupby(['year',aggr_level_colname,'epiweek','date'])['casos'].sum().reset_index()\n",
    "    df_cases_aggr                           = df_cases_aggr[['date','epiweek',aggr_level_colname,'year','casos']]\n",
    "    geocodes_harm                           = df_cases_raw[[aggr_level_colname,'geocode']].drop_duplicates()\n",
    "\n",
    "    df_population_raw                       = pd.read_csv(os.path.join(dir_data, 'datasus_population_2001_2024.csv.gz'))\n",
    "    df_population_raw                       = pd.merge(df_population_raw, geocodes_harm, on ='geocode')\n",
    "    df_population_aggr                      = df_population_raw.groupby(['year',aggr_level_colname])['population'].sum().reset_index()   \n",
    "\n",
    "    # Project 2025 population by adding the same difference to 2024\n",
    "    population_projection_df = df_population_aggr.copy()\n",
    "    population_projection_df        = population_projection_df.pivot(index=aggr_level_colname, columns='year', values='population')\n",
    "    population_projection_df['diff']= population_projection_df[2024] - population_projection_df[2023]\n",
    "    population_projection_df[2025]  = population_projection_df[2024] + population_projection_df['diff']\n",
    "    population_projection_df_2025   = population_projection_df[[2025]].reset_index()\n",
    "    population_projection_df_2025['year'] = 2025\n",
    "    population_projection_df_2025.rename(columns={2025: 'population'}, inplace=True)\n",
    "        # Match the original DataFrame format\n",
    "    population_projection_df_2025 = population_projection_df_2025[['year', aggr_level_colname, 'population']]\n",
    "        # Append to the original DataFrame\n",
    "    df_population_aggr_updated = pd.concat([df_population_aggr, population_projection_df_2025], ignore_index=True)\n",
    "    df_population_aggr_updated = df_population_aggr_updated.sort_values(by=['year', aggr_level_colname]).reset_index(drop=True)\n",
    "\n",
    "    df                                      = pd.merge(df_cases_aggr, df_population_aggr_updated, on = ['year',aggr_level_colname])\n",
    "    df['incidence']                         = df['casos'] / df['population'] * 10_000\n",
    "    df['date']                              = pd.to_datetime(df['date'])\n",
    "    df.rename(columns={'casos': 'cases'}, inplace=True)\n",
    "\n",
    "    # Climate data\n",
    "    df_climate_raw                          = pd.read_csv(os.path.join(dir_data, 'climate.csv.gz'))\n",
    "    df_climate_raw['date']                  = pd.to_datetime(df_climate_raw['date'])\n",
    "    df_climate_raw                          = pd.merge(df_climate_raw, geocodes_harm, on ='geocode')\n",
    "\n",
    "    # Area data\n",
    "    area_data_raw                           = pd.read_csv(os.path.join(dir_proj, 'land_coverage_cleaned.csv'))\n",
    "    area_data_raw                           = area_data_raw[['geocode', 'AREA_HECTARES']].drop_duplicates()\n",
    "    climate_area_raw                        = pd.merge(df_climate_raw, area_data_raw, on ='geocode')\n",
    "\n",
    "    exclude_cols                            = ['date', 'epiweek', 'geocode', aggr_level_colname, 'AREA_HECTARES']\n",
    "    climate_cols                            = [col for col in climate_area_raw.select_dtypes(include='number').columns if col not in exclude_cols]\n",
    "\n",
    "    def weighted_mean(group, value_cols, weight_col):\n",
    "        weights = group[weight_col]\n",
    "        # For each climate column, calculate weighted average using municipality's area\n",
    "        return pd.Series({\n",
    "            col: (group[col] * weights).sum() / weights.sum()\n",
    "            for col in value_cols\n",
    "        })\n",
    "\n",
    "    # Group by 'date' and aggregation level and apply weighted mean towards area\n",
    "    df_climate_aggr         = climate_area_raw.groupby(['date', aggr_level_colname]).apply(weighted_mean, climate_cols, 'AREA_HECTARES').reset_index()\n",
    "    df_climate_aggr['date'] = pd.to_datetime(df_climate_aggr['date'])\n",
    "    df                      = pd.merge(df, df_climate_aggr, on = ['date',aggr_level_colname])\n",
    "\n",
    "    # ocean climate data\n",
    "    df_ocean_climate                        = pd.read_csv(os.path.join(dir_data, 'ocean_climate_oscillations.csv.gz'))\n",
    "    df_ocean_climate['date']                = pd.to_datetime(df_ocean_climate['date'])\n",
    "\n",
    "    # dates from ocean climate data didn't entirely match with the other datasets. \n",
    "    df = pd.merge_asof(\n",
    "        df.sort_values('date'),\n",
    "        df_ocean_climate.sort_values('date'),\n",
    "        on        = 'date',\n",
    "        direction ='nearest',\n",
    "        tolerance = pd.Timedelta('7 days') \n",
    "    )\n",
    "    # environment data\n",
    "    df_enviroment_raw           = pd.read_csv(os.path.join(dir_data,'environmental_additions.csv'))\n",
    "\n",
    "    rename_environment_cols = {\n",
    "    'Coastal_Water_Body-sum_normalized'        :   'coastal',\n",
    "    'Continental_Water_Body-sum_normalized'    :   'water_body',\n",
    "    'Depressions-sum_normalized'               :   'depressions',\n",
    "    'Levels-sum_normalized'                    :   'levels',\n",
    "    'Mountains-sum_normalized'                 :   'mountains',\n",
    "    'Oceanic_Island-sum_normalized'            :   'oceanic_island',\n",
    "    'Plains-sum_normalized'                    :   'plains',\n",
    "    'Plates-sum_normalized'                    :   'plates',\n",
    "    'Plateus-sum_normalized'                   :   'plateus',\n",
    "    'Trays-sum_normalized'                     :   'trays',\n",
    "    'Railways-sum_normalized'                  :   'railways',\n",
    "    'Amazon_Forest-sum_normalized'             :   'amazon',\n",
    "    'Atlantic_Forest-sum_normalized'           :   'atlantic_forest',\n",
    "    'Caatinga-sum_normalized'                  :   'caatinga',\n",
    "    'Cerrado-sum_normalized'                   :   'cerrado',\n",
    "    'Pampa-sum_normalized'                     :   'pampa',\n",
    "    'Pantanal-sum_normalized'                  :   'pantanal',\n",
    "    'Altitude-mean'                            :   'altitude_mean',\n",
    "    'Altitude-sd'                              :   'altitude_sd',\n",
    "    'Altitude-min'                             :   'altitude_min',\n",
    "    'Altitude-max'                             :   'altitude_max'\n",
    "    }\n",
    "\n",
    "    df_enviroment_raw           = df_enviroment_raw.rename(columns = rename_environment_cols)\n",
    "    df_enviroment_raw           = pd.merge(df_enviroment_raw, area_data_raw, on = ['geocode'])\n",
    "    df_enviroment_raw           = pd.merge(df_enviroment_raw, geocodes_harm, on = ['geocode'])\n",
    "\n",
    "    exclude_cols                = ['geocode', aggr_level_colname, 'AREA_HECTARES']\n",
    "    environment_cols            = [col for col in df_enviroment_raw.select_dtypes(include='number').columns if col not in exclude_cols]\n",
    "\n",
    "\n",
    "    def weighted_mean(group, value_cols, weight_col):\n",
    "        weights = group[weight_col]\n",
    "        # For each climate column, calculate weighted average\n",
    "        return pd.Series({\n",
    "            col: (group[col] * weights).sum() / weights.sum()\n",
    "            for col in value_cols\n",
    "        })\n",
    "\n",
    "    # Group by 'date' and 'uf' and apply weighted mean weighted by municipality's area\n",
    "    df_environment_aggr = df_enviroment_raw.groupby([aggr_level_colname]).apply(weighted_mean, environment_cols, 'AREA_HECTARES').reset_index()\n",
    "\n",
    "    df = pd.merge(df, df_environment_aggr, on =aggr_level_colname)\n",
    "\n",
    "    data_division           = df_cases_raw[['date','train_1','target_1', 'train_2', 'target_2','train_3', 'target_3']]\n",
    "    data_division           = data_division.drop_duplicates().reset_index(drop = True)\n",
    "    data_division['date']   = pd.to_datetime(data_division['date'])\n",
    "    df                      = pd.merge(df, data_division, on = ['date'])\n",
    "\n",
    "    df['cases'] = df['cases'].astype(float)\n",
    "    return df\n",
    "\n",
    "def standardize_data(df, \n",
    "                     aggr_level,\n",
    "                     cheating,\n",
    "                     save_data,\n",
    "                     return_dfs ):\n",
    "    \"\"\"\n",
    "    takes a merged df, and based on given parameters splits data into train and validation data, \n",
    "    adds cheating data (or not), stores the data and returns the dataframes.\n",
    "\n",
    "    Cheating data is the data covering the gaps between train and val.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    remove_columns          = ['train_1','target_1', 'train_2', 'target_2','train_3', 'target_3']\n",
    "    df['population']        = df['population'].astype('float64')\n",
    "    cols_to_scale           = df.select_dtypes(include=['float']).columns.tolist()\n",
    "    train_dfs = []\n",
    "    val_dfs   = []\n",
    "\n",
    "    for ii in range(3):\n",
    "        # index 1-3\n",
    "        i       = ii + 1 \n",
    "        train_i = f'train_{i}'\n",
    "        val_i   = f'target_{i}'\n",
    "\n",
    "        train_i_df = df[df[train_i] == True].reset_index(drop=True)\n",
    "        val_i_df   = df[df[val_i] == True].reset_index(drop=True)\n",
    "\n",
    "        final_train_i_date = train_i_df['date'].max()\n",
    "        first_val_i_date   = val_i_df['date'].min()\n",
    "\n",
    "        if cheating:\n",
    "            cheating_data = df[(df['date'] < first_val_i_date) & (df['date'] > final_train_i_date)]\n",
    "            val_i_df = pd.concat([cheating_data,val_i_df])\n",
    "\n",
    "        # add sin / cos of week:\n",
    "        cols_to_scale = cols_to_scale + ['week_sin','week_cos']\n",
    "\n",
    "        train_i_df = add_week_features(train_i_df)\n",
    "        val_i_df   = add_week_features(val_i_df)\n",
    "\n",
    "        train = train_i_df.drop(columns = remove_columns)\n",
    "        val   = val_i_df.drop(columns = remove_columns)\n",
    "\n",
    "        train_std, std_params = pipeline_minmax_scaling(train, cols_to_scale)\n",
    "        val_std               = apply_minmax_scaling(val, cols_to_scale, std_params)\n",
    "\n",
    "        if return_dfs:\n",
    "            train_dfs.append(train_std)\n",
    "            val_dfs.append(val_std)\n",
    "\n",
    "        if save_data:\n",
    "            train_std.to_csv(dir_data + '/processed/' + f'train_std_{aggr_level}_{i}.csv', index=False)\n",
    "            val_std.to_csv(dir_data + '/processed/' + f'val_std_{aggr_level}_{i}.csv', index=False)\n",
    "\n",
    "            with open(dir_data + '/processed/' + f'scaling_{aggr_level}_{i}.json', 'w') as f:\n",
    "                json.dump(std_params, f)   \n",
    "\n",
    "    return train_dfs, val_dfs\n",
    "\n",
    "def merge_standardize_data(aggr_level:str  ='state',\n",
    "                           save:bool       = False,\n",
    "                           cheating:bool   = False,\n",
    "                           return_dfs:bool = False):\n",
    "    \"\"\"\n",
    "    Main orchestrator function to merge and standardize data\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    aggr_level: str\n",
    "        aggregation level. Supported values are: \"state\", \"maroregion\" and \"region\"\n",
    "    save: bool\n",
    "        whether or not to save the final standardized and split dataframes\n",
    "    cheating: bool\n",
    "        whether or not to include the data within the gap between training and validation data, in the validation dataset\n",
    "    return_dfs: bool\n",
    "        whether or not to return a list of the final train_dfs and a list of the final val_dfs.\n",
    "        If not required, put to False, as this will save memory.\n",
    "\n",
    "    Also see:\n",
    "    --------\n",
    "    load_and_merge_rawdata => loads, preprocesses and merges all datasets\n",
    "    standardize_data       => standardize training and validation sets separately\n",
    "    \"\"\"\n",
    "    \n",
    "    merged_data = load_and_merge_rawdata(aggr_level)\n",
    "\n",
    "    std_data    = standardize_data(merged_data, aggr_level, save_data = save, cheating = cheating, return_dfs = return_dfs)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    for level in ['state','region','macroregion']:\n",
    "        merge_standardize_data(aggr_level   = level,\n",
    "                            save         = True,\n",
    "                            cheating     = True,\n",
    "                            return_dfs   = False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
